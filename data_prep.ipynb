{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import itertools\n",
    "import datetime\n",
    "import hashlib\n",
    "import scipy.spatial.distance as ssd\n",
    "from sklearn.preprocessing import normalize\n",
    "import locale\n",
    "import numpy as np\n",
    "import pycountry\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as ss\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#STEP 1-ceating user_event_response matrix \n",
    "#We are first creating emply sets to hold all the unique users and events\n",
    "distinct_users = set()\n",
    "distinct_events = set()\n",
    "#Next we create empty dictionaries, one to hold all events presented to each user and another to hold all users each event is presented to.\n",
    "events_for_each_user = defaultdict(set)\n",
    "users_for_each_event = defaultdict(set)\n",
    "#The below lines of code fill the two sets and two dictionaries with respective contents as mentioned above\n",
    "for data in [\"event\\\\train.csv\", \"event\\\\test.csv\"]:\n",
    "  file = open(data, 'r')\n",
    "  file.readline().strip().split(\",\")\n",
    "  #print(f)\n",
    "  for line in file:\n",
    "    columns = line.strip().split(\",\")\n",
    "    distinct_users.add(columns[0])\n",
    "    distinct_events.add(columns[1])\n",
    "    events_for_each_user[columns[0]].add(columns[1])\n",
    "    users_for_each_event[columns[1]].add(columns[0])\n",
    "  file.close()\n",
    "#dok_matrix is used to generate a sparse matrix(to keep only those entries that are non zero as row index,column index,value)\n",
    "user_event_response = ss.dok_matrix((len(distinct_users), len(distinct_events)))\n",
    "#Creating two dictionaries to store users and events with their indices.\n",
    "user_index=dict()\n",
    "event_index = dict()\n",
    "#filling user_index with user:index\n",
    "for i, l in enumerate(distinct_users):\n",
    "  user_index[l] = i\n",
    "#dilling event_index with event:index\n",
    "for i, l in enumerate(distinct_events):\n",
    "  event_index[l] = i\n",
    "\n",
    "train = open(\"event\\\\train.csv\", 'r')\n",
    "#skip the first row\n",
    "train.readline()\n",
    "#Here we are filling the sparse matrix: the sparse matrix contains: for each event-user in the training data-> the index of the user \n",
    "#and the index of the event according to the user_index and event_index list followed by user's response to the event\n",
    "for l in train:\n",
    "  col = l.strip().split(\",\")\n",
    "  #print(col)\n",
    "  i = user_index[col[0]]\n",
    "  j = event_index[col[1]]\n",
    "  user_event_response[i, j] = int(col[4]) - int(col[5])\n",
    "#user's response to event will be 1 if he is interested , -1 is not interested , and 0 if neither.(but these 0's wont be written to the sparse matrix)\n",
    "train.close()\n",
    "#Now we are writing this sparse matrix to a mtx file\n",
    "sio.mmwrite(\"event\\\\user_event_response\", user_event_response)\n",
    "\n",
    "#________________________________________________________________________________________________________________________________________\n",
    "\n",
    "#STEP 2-finding unique user and event pairs\n",
    " \n",
    "distinct_user_pairs = set()\n",
    "distinct_event_pairs = set()\n",
    "# Now we will find all unique user pairs and event pairs\n",
    "#  These should be users who are linked via an event or events that are linked via a user in either the training or test sets. \n",
    "for e in distinct_events:\n",
    "  u_list = users_for_each_event[e]\n",
    "  if len(u_list) > 2:\n",
    "    distinct_user_pairs.update(itertools.combinations(u_list, 2))\n",
    "\n",
    "for u in distinct_users:\n",
    "      e_list = events_for_each_user[u]\n",
    "      if len(e_list) > 2:\n",
    "        distinct_event_pairs.update(itertools.combinations(e_list, 2))\n",
    "#________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STEP-3 : Encoding the variables\n",
    "\n",
    "# Locale is a column in users.csv. But it is categorical. We need to encode this. But we know that with new entries, there could \n",
    "#be new locales, so in order to encode it properly accounting all possible locales, we create a dictionary that contains all possible\n",
    "#locales mapped to unique id's. We use the locale library for this\n",
    "locales_id = defaultdict(int)\n",
    "for i, loc in enumerate(locale.locale_alias.keys()):\n",
    "  locales_id[loc] = i + 1\n",
    "#print(localeIdMap)\n",
    "\n",
    "# Similarly , we need to encode the location,again with new entries, there could \n",
    "#be new countries, so in order to encode it properly accounting all possible countries, we create a dictionary that contains all possible\n",
    "#countries mapped to unique id's. We use the pycountry library for this\n",
    "# load countries\n",
    "country_id = defaultdict(int)\n",
    "for i, country in enumerate(pycountry.countries):\n",
    "  #print(c)\n",
    "  country_id[country.name.lower()] = i + 1 \n",
    "#print(countryIdMap)\n",
    "#print(ctryIdx)\n",
    "\n",
    "#To encode gender: male->1 female-> 2\n",
    "gender_id = defaultdict(int, {\"male\":1, \"female\":2})\n",
    "#print(genderIdMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Following are the functions to encode each of the six variables in users.csv and events.csv\n",
    "#locale is encoded using the locales_id dictionary created above\n",
    "def encode_locale(l):\n",
    "    return locales_id[l.lower()]\n",
    "#gender is encoded using the gender_id dictionary created above\n",
    "def encode_gender(g):\n",
    "    return gender_id[g]\n",
    "\n",
    "def encode_birth(b):\n",
    "    try:\n",
    "      return 0 if b == \"None\" else int(b)\n",
    "    except:\n",
    "      return 0\n",
    "\n",
    "def encode_joined_month(d):\n",
    "    val = datetime.datetime.strptime(d, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    return \"\".join([str(val.year), str(val.month)])\n",
    "\n",
    "def encode_timezone(t):\n",
    "    try:\n",
    "      return int(t)\n",
    "    except:\n",
    "      return 0\n",
    "\n",
    "#location is encoded using the country_id dictionary created above\n",
    "def encode_country(c):\n",
    "    if (isinstance(c, str) and len(c.strip()) > 0 and c.rfind(\"  \") > -1):\n",
    "      #print(location[location.rindex(\"  \"):].lower())\n",
    "      return country_id[c[c.rindex(\"  \") + 2:].lower()]\n",
    "    else:\n",
    "      return 0\n",
    "\n",
    "def feature_hash(value):\n",
    "    if len(value.strip()) == 0:\n",
    "      return -1\n",
    "    else:\n",
    "      return int(hashlib.sha224(value).hexdigest()[0:4], 16)\n",
    "\n",
    "def float_value(value):\n",
    "    if len(value.strip()) == 0:\n",
    "      return 0.0\n",
    "    else:\n",
    "      return float(value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#STEP 4 : creating user matrix with index of each user and their preprocessed variables\n",
    "n = len(user_index.keys())\n",
    "#print(n)\n",
    "file = open(\"event\\\\users.csv\", 'r')\n",
    "columns = file.readline().strip().split(\",\")\n",
    "#print(len(columns))\n",
    "user_mat = ss.dok_matrix((n, len(columns) - 1)) \n",
    "for l in file:\n",
    "  col = l.strip().split(\",\")\n",
    "  # consider the users if they are in train.csv\n",
    "  if col[0] in user_index:\n",
    "    i = user_index[col[0]]\n",
    "    user_mat[i, 0] = encode_locale(col[1])\n",
    "    #print(encode_locale(col[1]))\n",
    "    user_mat[i, 1] = encode_birth(col[2])\n",
    "    user_mat[i, 2] = encode_gender(col[3])\n",
    "    user_mat[i, 3] = encode_joined_month(col[4])\n",
    "    user_mat[i, 4] = encode_country(col[5])\n",
    "    user_mat[i, 5] = encode_timezone(col[6])\n",
    "file.close()\n",
    "\n",
    "sio.mmwrite(\"event\\\\temp\", user_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#STEP 5: finding similarity between each pair of users\n",
    "#Normalise the user matrix\n",
    "user_mat = normalize(user_mat, norm=\"l1\", axis=0, copy=False)\n",
    "sio.mmwrite(\"event\\\\user_matrix\", user_mat)\n",
    "# calculate the user similarity matrix\n",
    "similarity_mat = ss.dok_matrix((n, n))\n",
    "#diagonal elements are one as the similarity between a user and himself is 1\n",
    "for i in range(0, n):\n",
    "  similarity_mat[i, i] = 1.0\n",
    "# for each unique pair in distinct_user_pairs, the correlation similarity measure is found by comparing the feature vectors for the users\n",
    "#and that entry is stored in the similarity matrix\n",
    "#this matrix is then written into the similaritymatrix.mtx file\n",
    "for a,b in distinct_user_pairs:\n",
    "  i = user_index[a]\n",
    "  j = user_index[b]\n",
    "  if similarity_mat[i,j]==0.0:\n",
    "    coeff = ssd.correlation(user_mat.getrow(i).todense(),\n",
    "      user_mat.getrow(j).todense())\n",
    "    similarity_mat[i, j] = coeff\n",
    "    similarity_mat[j, i] = coeff\n",
    "sio.mmwrite(\"event\\\\similaritymatrix\", similarity_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
